{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vllm instructor openai pydantic\n",
    "# !pip install xformers==0.0.27 vllm-flash-attn==v2.5.9.post1 vllm==0.5.2 librosa transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450dfd5f-4e50-4032-acfd-cac95b85e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from instructor import from_openai\n",
    "from instructor.mode import Mode\n",
    "from openai import OpenAI\n",
    "import weave\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd72fdab-5fbd-4942-8e17-5384be743a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: praveenhm2.\n",
      "View Weave data at https://wandb.ai/praveenhm2/openrouter-chat/weave\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpraveenhm2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/praveen/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/praveen/github/flow-judge/wandb/run-20240922_135630-wdqcdbyo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praveenhm2/openrouter-chat/runs/wdqcdbyo' target=\"_blank\">chocolate-violet-20</a></strong> to <a href='https://wandb.ai/praveenhm2/openrouter-chat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praveenhm2/openrouter-chat' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praveenhm2/openrouter-chat/runs/wdqcdbyo' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat/runs/wdqcdbyo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weave.init('openrouter-chat')\n",
    "\n",
    "# Add wandb initialization\n",
    "wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    wandb.login(key=wandb_key)\n",
    "    wandb.init(project=\"openrouter-chat\")\n",
    "else:\n",
    "    print(\"WANDB_API_KEY not found. Skipping wandb initialization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d1f4b0-537c-484c-9a9a-2df860d72a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with Instructor\n",
    "openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "openrouter_base_url = \"https://openrouter.ai/api/v1\"\n",
    "client = from_openai(OpenAI(api_key=os.environ.get(\"OPENROUTER_API_KEY\"),base_url=\"https://openrouter.ai/api/v1\"),mode=Mode.JSON)\n",
    "# client = from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72909cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Score(BaseModel):\n",
    "    \"\"\"A score includes the value and description used to determine the metric being evaluated.\"\"\"\n",
    "    score: int = Field(..., description=\"The score of the user\")\n",
    "    determination: str = Field(..., description=\"The description that determines the score.\",\n",
    "                               examples=[\"Score 0 - There is no factual evidence to support the claim\",\n",
    "                                         \"Score 1 - There is factual evidence to support the claim\"])\n",
    "\n",
    "class Metric(BaseModel):\n",
    "    \"\"\"A metric is a set of criteria with numerical scores and descriptions used to evaluate the output of a language model. It is based on the prompt input.\"\"\"\n",
    "    metric_name: str = Field(..., description=\"The title of the metric.\")\n",
    "    metric_desc: str = Field(..., description=\"The description of the metric being used.\")\n",
    "    metric_type: str = Field(..., description=\"The type of metric being described as either a binary or graded metric.\",\n",
    "                      examples=[\"Graded (0-5)\", \"Binary (Pass/Fail)\"])\n",
    "    criteria: list[Score] = Field(..., description=\"Based on the metric, returns the set of scores that will be used to evaluate the metric.\")\n",
    "\n",
    "\n",
    "class FullMetric(BaseModel):\n",
    "    \"\"\"A list of components to evaluate an Language Model Response, which includes the description of the individual metrics, the set of criteria and scores per metric.\"\"\"\n",
    "    description: str = Field(..., description=\"A detailed description of the metric, describing the purpose and vision of an evaluation criteria.\")\n",
    "    nlp_tasks: list[str] = Field(..., description=\"A list of NLP tasks that are distinct and domain-specific\",\n",
    "                                 examples=\"Clinical NER, Text Generation Storytelling for Children, QA RAG\")\n",
    "    metric: list[Metric] = Field(..., min_length=3, max_length=10, description=\"The metric being described\",\n",
    "                                 examples=[\n",
    "                                     \"Metric 1: The response includes a factual error and scores using a Binary Criteria. Criteria: The response includes a factual error. Score: 0\",\n",
    "                                     \"Metric 2: The response is not descriptive and scores using a 5 point Likert Scale Criteria. Criteria: The response is not descriptive. Score: 1\"\n",
    "                                 ]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7236f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the travel agent to use\n",
    "from textwrap import dedent\n",
    "\n",
    "def generate_metric(user_prompt, llm_response):\n",
    "    system_content = dedent(\"\"\"\n",
    "    Instructions:\n",
    "    You are an AI assistant tasked with evaluating a response generated by an LLM (Large Language Model). \n",
    "    Responses and descriptions need to be concrete, and well-structured, effectively highlighting key points that lead towards an actionable response by the reviewer. \n",
    "    The evaluation must be tailored to the specific NLP tasks and domain expertise required by the user's prompt in a specific manner.\n",
    "    Follow the steps below to produce a comprehensive, expert-level assessment using dynamically generated metrics that are highly relevant to the task and domain at hand.\n",
    "    You have the full control over the creativity of the output, so long as it is non-inferior to the current examples listed.\n",
    "    Step 1: Analyze the Task and Domain\n",
    "    1. Identify the NLP Task Type:\n",
    "    Examine the user prompt and the LLM's response to determine the specific NLP task involved. Possible tasks include:\n",
    "    Question Answering (QA)\n",
    "    Named Entity Recognition (NER)\n",
    "    Creative Generation (e.g., storytelling, poem writing)\n",
    "    Summarization\n",
    "    Translation\n",
    "    Dialogue Generation\n",
    "    Information Extraction\n",
    "    Classification\n",
    "    2. Determine the Required Domain Expertise:\n",
    "    Identify the subject matter and domain-specific knowledge needed to address the prompt effectively. Possible domains include:\n",
    "    Medicine\n",
    "    Law\n",
    "    Literature\n",
    "    Computer Science\n",
    "    Finance\n",
    "    Psychology\n",
    "    Education\n",
    "    ---\n",
    "    Step 2: Generate Customized Evaluation Metrics\n",
    "    Based on the identified NLP task and domain expertise, generate a set of evaluation metrics that are both task-specific and domain-specific. Ensure that these metrics are defined clearly and are appropriate for an expert in the field to use as a standard.\n",
    "    A. Task-Specific Metrics\n",
    "    Define metrics that are standard for evaluating the identified NLP task.\n",
    "    Examples:\n",
    "    For Question Answering:\n",
    "    Accuracy of Answer\n",
    "    Completeness\n",
    "    Relevance\n",
    "    For NER:\n",
    "    Precision\n",
    "    Recall\n",
    "    F1 Score\n",
    "    For Creative Generation:\n",
    "    Originality\n",
    "    Creativity\n",
    "    Emotional Impact\n",
    "    B. Domain-Specific Metrics\n",
    "    Define metrics that reflect the standards and expectations of the identified domain.\n",
    "    Examples:\n",
    "    For Medicine:\n",
    "    Clinical Accuracy\n",
    "    Use of Medical Terminology\n",
    "    Adherence to Ethical Guidelines\n",
    "    For Law:\n",
    "    Legal Accuracy\n",
    "    Citation of Relevant Laws\n",
    "    Logical Consistency in Arguments\n",
    "    ---\n",
    "    Step 3: Evaluate the LLM's Response Using the Generated Metrics\n",
    "    For each metric:\n",
    "    1. Define the Metric:\n",
    "    Provide a clear and concise definition so that it is understood how the evaluation will be conducted.\n",
    "    2. Evaluation Method:\n",
    "    Graded Metrics: Use a numerical scale (e.g., 0-5) where appropriate.\n",
    "    Binary Metrics: Use Pass/Fail, Yes/No, or Compliant/Non-Compliant where a binary assessment is more suitable.\n",
    "    3. Assess the Response:\n",
    "    Apply the metric to the LLM's response.\n",
    "    Provide specific examples or evidence from the response to support your assessment.\n",
    "    4. Provide a Score or Judgment:\n",
    "    Assign a score or make a binary judgment as defined.\n",
    "    ---\n",
    "    Step 4: Provide an Overall Assessment and Recommendations\n",
    "    Summarize the Overall Quality:\n",
    "    Highlight the main strengths and weaknesses observed in the response.\n",
    "    Final Recommendations:\n",
    "    Offer actionable suggestions for improvement.\n",
    "    Suggest any additional resources or corrections needed.\n",
    "    ---\n",
    "    Template for the Evaluation Report:\n",
    "    ---\n",
    "    User Prompt:\n",
    "    [Insert User Prompt Here]\n",
    "    LLM's Response:\n",
    "    [Insert LLM's Response Here]\n",
    "    ---\n",
    "    Analysis:\n",
    "    1. Identified NLP Task Type: [Specify the task]\n",
    "    2. Identified Domain Expertise Required: [Specify the domain]\n",
    "    ---\n",
    "    Customized Evaluation Metrics:\n",
    "    Metric 1: [Name of Metric]\n",
    "    Type: [Graded (0-5) or Binary (Pass/Fail)]\n",
    "    Definition: [Provide a clear definition]\n",
    "    Assessment:\n",
    "    [Apply the metric to the response, citing specific examples]\n",
    "    Score/Judgment: [Provide the score or Pass/Fail]\n",
    "    (Repeat for each metric)\n",
    "    ---\n",
    "    Overall Assessment:\n",
    "    Summary of Findings:\n",
    "    [Summarize the key points from the evaluation]\n",
    "    Strengths:\n",
    "    [List what the response did well]\n",
    "    Areas for Improvement:\n",
    "    [List what can be improved, with suggestions]\n",
    "    Final Recommendations:\n",
    "    [Provide actionable advice]\n",
    "    ---\n",
    "    Example Evaluation Report:\n",
    "    (Below is an illustrative example using placeholders. Replace with actual content.)\n",
    "    ---\n",
    "    User Prompt:\n",
    "    \"Explain the process of mitosis in human cells.\"\n",
    "    LLM's Response:\n",
    "    \"Mitosis is the process by which a cell divides into two new cells. It consists of phases called prophase, metaphase, anaphase, and telophase. During mitosis, DNA replicates, and the cell splits equally.\"\n",
    "    ---\n",
    "    Analysis:\n",
    "    1. Identified NLP Task Type: Expository Answering\n",
    "    2. Identified Domain Expertise Required: Cell Biology\n",
    "    ---\n",
    "    Customized Evaluation Metrics:\n",
    "    Metric 1: Scientific Accuracy\n",
    "    Type: Graded (0-5)\n",
    "    Definition: Evaluates the correctness of biological facts presented.\n",
    "    Assessment:\n",
    "    The response correctly identifies mitosis as a cell division process.\n",
    "    It mentions the phases but omits cytokinesis.\n",
    "    It incorrectly states that DNA replicates during mitosis (DNA replication occurs during interphase).\n",
    "    Score: 3/5\n",
    "    Metric 2: Use of Terminology\n",
    "    Type: Binary (Pass/Fail)\n",
    "    Definition: Assesses correct use of biological terms.\n",
    "    Assessment:\n",
    "    Terms like \"prophase,\" \"metaphase,\" \"anaphase,\" and \"telophase\" are used correctly.\n",
    "    Misuse of \"DNA replicates during mitosis.\"\n",
    "    Judgment: Fail\n",
    "    Metric 3: Completeness\n",
    "    Type: Graded (0-5)\n",
    "    Definition: Measures how thoroughly the response covers the process.\n",
    "    Assessment:\n",
    "    Lacks mention of interphase and cytokinesis.\n",
    "    Does not describe what happens in each phase.\n",
    "    Score: 2/5\n",
    "    ---\n",
    "    Overall Assessment:\n",
    "    Summary of Findings:\n",
    "    The response demonstrates a basic understanding of mitosis but contains factual inaccuracies and omissions.\n",
    "    Strengths:\n",
    "    Correctly lists the main phases of mitosis.\n",
    "    Areas for Improvement:\n",
    "    Correct the misconception about DNA replication timing.\n",
    "    Include details about each phase.\n",
    "    Mention cytokinesis and its role.\n",
    "    Final Recommendations:\n",
    "    Revise the response to correct factual errors.\n",
    "    Expand on each phase to enhance completeness.\n",
    "    Ensure all biological terms are used accurately.\n",
    "    ---\n",
    "    Note: This evaluation is tailored to the specific task (explaining a biological process) and domain (cell biology), using metrics that are relevant and meaningful to experts in the field.\n",
    "    ---\n",
    "    Guidelines for Using This Prompt:\n",
    "    Adaptability: The prompt is designed to be flexible. The evaluator dynamically generates metrics based on the task and domain identified.\n",
    "    Specificity: By focusing on task-specific and domain-specific metrics, the evaluation becomes more precise and valuable.\n",
    "    Expert-Level Assessment: The metrics and evaluation criteria aim to reflect the standards expected by professionals in the relevant field.\n",
    "    Actionable Feedback: Providing detailed assessments and recommendations helps improve future LLM responses.                        \n",
    "    \"\"\")\n",
    "\n",
    "    # Use the Instructor framework for function calling\n",
    "    evaluation = client.chat.completions.create(\n",
    "        # model=\"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "        # model=\"openai/gpt-4o\",\n",
    "        model=\"gpt-4o\",\n",
    "        response_model=FullMetric,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": f\"User Prompt: {user_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"LLM's Response: {llm_response}\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93866f81-34bf-4e72-b776-9b7c210fcd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/praveenhm2/openrouter-chat/r/call/01921b94-2b66-7b40-9e70-b1d671655b47\n",
      "what is prior auth? Prior authorization, often abbreviated as \"prior auth\" or \"PA,\" is a health care process that requires providers to obtain approval from a patient's health insurance company before prescribing a specific medication, treatment, or service. This approval process ensures that the proposed intervention is medically necessary and covered under the patient's insurance plan.\n",
      "\n",
      "The steps typically involved in prior authorization include:\n",
      "\n",
      "1. **Provider's Submission**: The health care provider submits a request to the insurance company, providing detailed information about the patient's medical condition and the proposed treatment or service.\n",
      "\n",
      "2. **Review**: The insurance company reviews the submitted information against their coverage policies, clinical guidelines, and the patient's health plan benefits.\n",
      "\n",
      "3. **Decision**: The insurance company either approves or denies the request. If approved, the patient can proceed with the treatment or service with the assurance that it will be covered by their insurance. If denied, the provider and patient may need to explore alternative treatments or appeal the decision.\n",
      "\n",
      "Prior authorization is used to manage costs, ensure the appropriate use of medical services, and prevent unnecessary or potentially harmful treatments. However, it can also lead to delays in care and additional administrative work for providers.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "def call_llm(input_prompt):\n",
    "    client2 = OpenAI(api_key=os.environ.get(\"OPENROUTER_API_KEY\"),base_url=\"https://openrouter.ai/api/v1\")\n",
    "    response = client2.chat.completions.create(\n",
    "        # model=\"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "        # model=\"openai/gpt-4o\",\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": input_prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content, input_prompt\n",
    "llm_output, init_prompt = call_llm(\"what is prior auth?\")\n",
    "print(init_prompt, llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf728c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/praveenhm2/openrouter-chat/r/call/01921b85-91eb-79b0-a661-be07d83cd103\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Given a Hypertensive CKD patient with Heart Failure and declining kidney function and hypokalemia, provide a care plan to improve their health.\"\n",
    "sample_response = \"The patient should receive a beta blockers, low-salt diet, diuretics, and potassium supplements. No further action is needed because this already addresses all their needs.\"\n",
    "\n",
    "generated_metric = generate_metric(sample_prompt, sample_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83be082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# # rprint(generated_metric)\n",
    "\n",
    "# for m in generated_metric.metric:\n",
    "#     print(\"Metric: \", m.metric_name)\n",
    "#     print(\"Metric Type: \", m.metric_type)\n",
    "#     for c in m.criteria:\n",
    "#         print(\"Score: \", c.score)\n",
    "#         print(\"Determination: \", c.determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9f24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.metrics import list_all_metrics\n",
    "\n",
    "# list_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b7ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.models.model_factory import ModelFactory\n",
    "# from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "# from flow_judge.metrics import RESPONSE_CORRECTNESS_BINARY\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# # Create a model using ModelFactory\n",
    "# # model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\")\n",
    "# model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# # Initialize the judge\n",
    "# judge = FlowJudge(\n",
    "#     metric=RESPONSE_CORRECTNESS_BINARY,\n",
    "#     model=model\n",
    "# )\n",
    "\n",
    "# # Prepare evaluation input\n",
    "# eval_input = EvalInput(\n",
    "#     inputs=[{\"question\": \"What is the capital of France?\"}],\n",
    "#     output=\"The capital of France is Paris.\"\n",
    "# )\n",
    "\n",
    "# # Perform evaluation\n",
    "# result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342c17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# rprint(\"Score: \", result.score)\n",
    "# rprint(\"Feedback: \", result.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ceec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.models.model_factory import ModelFactory\n",
    "# from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "# from flow_judge.metrics import CustomMetric, RubricItem\n",
    "\n",
    "\n",
    "# # # model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\")\n",
    "# model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# custom_metric = CustomMetric(\n",
    "#     name=\"My Custom Metric\",\n",
    "#     criteria=\"Evaluate based on X, Y, and Z.\",\n",
    "#     rubric=[\n",
    "#         RubricItem(score=0, description=\"Poor performance\"),\n",
    "#         RubricItem(score=1, description=\"Good performance\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# judge = FlowJudge(\n",
    "#     metric=custom_metric,\n",
    "#     model=model\n",
    "# )\n",
    "\n",
    "# eval_input = EvalInput(\n",
    "#     inputs=[{\"question\": sample_prompt}],\n",
    "#     output=sample_response\n",
    "# )\n",
    "\n",
    "# result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4828fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# rprint(\"Score: \", result.score)\n",
    "# rprint(\"Feedback: \", result.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0460cd2-4b59-4fb9-bd9a-d6af2a970923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# # rprint(generated_metric)\n",
    "\n",
    "# for m in generated_metric.metric:\n",
    "#     print(\"Metric: \", m.metric)\n",
    "#     print(\"Metric Type: \", m.metric_type)\n",
    "#     for c in m.criteria:\n",
    "#         print(\"Score: \", c.score)\n",
    "#         print(\"Determination: \", c.determination)\n",
    "\n",
    "# custom_metric = CustomMetric(\n",
    "#     name=\"My Custom Metric\",\n",
    "#     criteria=\"Evaluate based on X, Y, and Z.\",\n",
    "#     rubric=[\n",
    "#         RubricItem(score=0, description=\"Poor performance\"),\n",
    "#         RubricItem(score=1, description=\"Good performance\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e0f294-6288-4815-8fbd-73ef4b5112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_dict = generated_metric.model_dump()\n",
    "# for m in metric_dict['metric']:\n",
    "#     # rprint(m['metric_name'])\n",
    "#     rprint(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "029e3ee0-8ade-455d-bee2-9dc771ab2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-22 13:56:41,689\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-22 13:56:42 config.py:383] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-22 13:56:42 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='flowaicom/Flow-Judge-v0.1', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 09-22 13:56:43 model_runner.py:997] Starting to load model flowaicom/Flow-Judge-v0.1...\n",
      "INFO 09-22 13:56:43 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.87it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 13:56:44 model_runner.py:1008] Loading model weights took 7.1659 GB\n",
      "INFO 09-22 13:56:45 gpu_executor.py:122] # GPU blocks: 2268, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.models.model_factory import ModelFactory\n",
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# Iterate through the list of metrics from FullMetric and create a list of judges\n",
    "judges = []\n",
    "for metric in generated_metric.metric:\n",
    "    # Convert the criteria into RubricItems\n",
    "    rubric = [RubricItem(score=score.score, description=score.determination) for score in metric.criteria]\n",
    "    # Create a custom metric\n",
    "    custom_metric = CustomMetric(\n",
    "        name=metric.metric_name+\" \"+metric.metric_type,\n",
    "        criteria=metric.metric_desc,\n",
    "        rubric=rubric\n",
    "    )\n",
    "    # Initialize the FlowJudge object for each custom metric\n",
    "    judge = FlowJudge(\n",
    "        metric=custom_metric,\n",
    "        model=model  # Assuming the model is pre-initialized\n",
    "    )\n",
    "    # Add the judge to the list\n",
    "    judges.append(judge)\n",
    "# Evaluate the sample input\n",
    "eval_input = EvalInput(\n",
    "    inputs=[{\"question\": sample_prompt}],\n",
    "    output=sample_response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "679bfaf6-39d5-433a-b6a6-91dd3cbd2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:04<00:00,  4.98s/it, est. speed input: 149.59 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Clinical Accuracy Graded (0-5)\n",
      "Score: 3\n",
      "Feedback: The response provided contains significant inaccuracies in medical recommendations for a patient with hypertensive CKD, heart failure, declining kidney function, and hypokalemia. \n",
      "\n",
      "1. **Beta Blockers**: While beta blockers are generally recommended for heart failure, they must be used cautiously in patients with CKD. The response does not specify the need for careful monitoring of kidney function and potential dose adjustments.\n",
      "2. **Low-Salt Diet**: This is appropriate for managing hypertension and heart failure.\n",
      "3. **Diuretics**: This recommendation is problematic. In a patient with declining kidney function, the use of diuretics must be carefully considered to avoid further kidney damage. The response does not address this critical aspect.\n",
      "4. **Potassium Supplements**: This is inappropriate for a patient with hypokalemia. Potassium supplements could worsen the hypokalemia and potentially lead to dangerous cardiac arrhythmias.\n",
      "\n",
      "The response fails to address the complexity of managing multiple comorbidities and does not adhere to clinical guidelines comprehensively. It lacks important clinical considerations such as careful monitoring of kidney function, potential dose adjustments, and the need for a more tailored approach to managing electrolyte imbalances.\n",
      "\n",
      "Therefore, the response is mostly accurate but lacks some important clinical considerations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:04<00:00,  4.05s/it, est. speed input: 159.73 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Use of Medical Terminology Binary (Pass/Fail)\n",
      "Score: 1\n",
      "Feedback: The output demonstrates a basic understanding of medical terminology relevant to the patient's condition. The terms used, such as \"beta blockers,\" \"low-salt diet,\" \"diuretics,\" and \"potassium supplements,\" are appropriate and correctly applied to the patient's complex medical situation. These terms are directly related to the management of hypertension, chronic kidney disease (CKD), and heart failure, which are the primary concerns in this case.\n",
      "\n",
      "However, the statement \"No further action is needed because this already addresses all their needs\" is overly simplistic and potentially dangerous given the complexity of the patient's condition. In reality, such a patient would require a highly individualized and comprehensive care plan developed by a multidisciplinary team, including careful monitoring and adjustments to medications, diet, and other treatments.\n",
      "\n",
      "While the medical terminology used is correct and appropriate, the overall message lacks the depth and specificity required for managing such a complex case. The care plan should include more detailed considerations for the patient's declining kidney function and hypokalemia, as well as the interplay between hypertension, CKD, and heart failure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:05<00:00,  5.20s/it, est. speed input: 142.08 toks/s, outpu"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Comprehensiveness Graded (0-5)\n",
      "Score: 0\n",
      "Feedback: The response fails to comprehensively address the complex condition of the patient. While it mentions some relevant treatments like beta blockers, low-salt diet, diuretics, and potassium supplements, it overlooks several critical aspects of care for a patient with hypertensive CKD, heart failure, declining kidney function, and hypokalemia.\n",
      "\n",
      "The care plan lacks depth in several areas:\n",
      "1. It doesn't address the specific needs of a patient with both CKD and heart failure, such as careful fluid management and avoiding certain medications that could worsen kidney function.\n",
      "2. The response doesn't mention the need for regular monitoring of kidney function and electrolytes, which is crucial for this patient.\n",
      "3. It doesn't address the potential need for adjustments in medication dosages due to declining kidney function.\n",
      "4. The plan doesn't consider the potential need for more advanced treatments like ACE inhibitors or ARBs, which can be beneficial in CKD and heart failure.\n",
      "5. It doesn't mention the importance of patient education and lifestyle modifications beyond diet.\n",
      "6. The response doesn't address the potential need for referral to a nephrologist or cardiologist for specialized care.\n",
      "\n",
      "Overall, while the response touches on some relevant treatments, it fails to provide a comprehensive and detailed care plan for a patient with such a complex condition. It lacks the depth and consideration of all critical aspects required for optimal care.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation for each judge\n",
    "for judge in judges:\n",
    "    result = judge.evaluate(eval_input)\n",
    "    print(f\"Metric: {judge.metric.name}\")\n",
    "    print(f\"Score: {result.score}\")\n",
    "    print(f\"Feedback: {result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a65b00e-4bae-498a-9087-e15f29d62805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(generated_metric, sample_prompt, sample_response):\n",
    "        # Iterate through the list of metrics from FullMetric and create a list of judges\n",
    "    judges = []\n",
    "    for metric in generated_metric.metric:\n",
    "        # Convert the criteria into RubricItems\n",
    "        rubric = [RubricItem(score=score.score, description=score.determination) for score in metric.criteria]\n",
    "        # Create a custom metric\n",
    "        custom_metric = CustomMetric(\n",
    "            name=metric.metric_name+\" \"+metric.metric_type,\n",
    "            criteria=metric.metric_desc,\n",
    "            rubric=rubric\n",
    "        )\n",
    "        # Initialize the FlowJudge object for each custom metric\n",
    "        judge = FlowJudge(\n",
    "            metric=custom_metric,\n",
    "            model=model  # Assuming the model is pre-initialized\n",
    "        )\n",
    "        # Add the judge to the list\n",
    "        judges.append(judge)\n",
    "    # Evaluate the sample input\n",
    "    eval_input = EvalInput(\n",
    "        inputs=[{\"question\": sample_prompt}],\n",
    "        output=sample_response\n",
    "    )\n",
    "    # Perform evaluation for each judge\n",
    "    for judge in judges:\n",
    "        result = judge.evaluate(eval_input)\n",
    "        print(f\"Metric: {judge.metric.name}\")\n",
    "        print(f\"Score: {result.score}\")\n",
    "        print(f\"Feedback: {result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fc34ff1-c9b0-4f55-915e-e99c91f4405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/praveenhm2/openrouter-chat/r/call/01921b99-9f95-71c2-8431-21a952c38c18\n",
      "🍩 https://wandb.ai/praveenhm2/openrouter-chat/r/call/01921b99-ab7c-7cc2-bd29-32005b7e41df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.81s/it, est. speed input: 330.60 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Accuracy of Answer Graded (0-5)\n",
      "Score: 5\n",
      "Feedback: The response provided is generally accurate and comprehensive in explaining what prior authorization is. It correctly defines prior authorization as a process used by health insurance companies to determine coverage for procedures, services, or medications before they are provided to the patient. The explanation includes key points such as the necessity of the treatment, the process of submitting a request to the insurance company, the approval process, the timeframe, and the impact on care.\n",
      "\n",
      "There are no significant factual errors in the response. The information is presented clearly and logically, covering the essential aspects of prior authorization. The response also includes practical advice on understanding insurance plans and working with healthcare providers, which adds value to the explanation.\n",
      "\n",
      "Given the accuracy and completeness of the information, the response meets the highest standard of the scoring rubric.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.61s/it, est. speed input: 353.96 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Completeness Graded (0-5)\n",
      "Score: 4\n",
      "Feedback: The response provides a comprehensive explanation of prior authorization, covering its definition, purpose, process, and impact on patient care. It addresses key aspects such as necessity, process, approval, timeframe, and impact on care. The response also includes a numbered list of points for clarity, which enhances its thoroughness.\n",
      "\n",
      "However, there are a few minor details that could have been included for a perfect score. For instance, the response could have mentioned specific challenges or common issues encountered during the prior authorization process. Additionally, it could have provided some examples of treatments or medications that typically require prior authorization.\n",
      "\n",
      "Overall, the response is detailed and covers most of the relevant information, but it lacks a few minor points that would make it fully comprehensive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.10s/it, est. speed input: 444.92 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Clarity Graded (0-5)\n",
      "Score: 5\n",
      "Feedback: The response provided is clear, well-structured, and easy to understand. It begins with a concise definition of prior authorization, followed by a detailed explanation that includes key points and subheadings. The language used is straightforward and accessible, making it easy for readers to grasp the concept and its implications. The structure, with bullet points and numbered lists, enhances readability and helps organize the information logically. There are no noticeable issues with structure or language that would hinder comprehension. Overall, the response meets the highest standard of clarity and readability as per the evaluation criteria.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.51s/it, est. speed input: 373.94 toks/s, outpu"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Relevance Graded (0-5)\n",
      "Score: 5\n",
      "Feedback: The response provided is entirely relevant to the question about prior authorization. It clearly defines what prior authorization is, explains the process, and outlines key points such as necessity, process, approval, timeframe, and impact on care. The information is directly related to the question and stays on topic throughout, providing a comprehensive answer without any significant off-topic information.\n",
      "\n",
      "The response effectively addresses the core aspects of prior authorization, including its purpose, the steps involved, and its implications for patient care. It also offers practical advice on navigating the process, which adds value to the answer.\n",
      "\n",
      "Overall, the response is well-structured, informative, and directly relevant to the question asked, making it a thorough and appropriate answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is prior auth?\"\n",
    "response, prompt = call_llm(prompt)\n",
    "lazy_metric = generate_metric(prompt, response)\n",
    "lazy_eval = generate_eval(lazy_metric, prompt,response)\n",
    "lazy_eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
