{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37f8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vllm instructor openai pydantic\n",
    "# !pip install xformers==0.0.27 vllm-flash-attn==v2.5.9.post1 vllm==0.5.2 librosa transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450dfd5f-4e50-4032-acfd-cac95b85e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from instructor import from_openai\n",
    "from instructor.mode import Mode\n",
    "from openai import OpenAI\n",
    "import weave\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd72fdab-5fbd-4942-8e17-5384be743a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zigjme47) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-waterfall-18</strong> at: <a href='https://wandb.ai/praveenhm2/openrouter-chat/runs/zigjme47' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat/runs/zigjme47</a><br/> View project at: <a href='https://wandb.ai/praveenhm2/openrouter-chat' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240922_133428-zigjme47/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zigjme47). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/praveen/github/flow-judge/wandb/run-20240922_133442-zqnkb3wv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/praveenhm2/openrouter-chat/runs/zqnkb3wv' target=\"_blank\">cerulean-sunset-19</a></strong> to <a href='https://wandb.ai/praveenhm2/openrouter-chat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/praveenhm2/openrouter-chat' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/praveenhm2/openrouter-chat/runs/zqnkb3wv' target=\"_blank\">https://wandb.ai/praveenhm2/openrouter-chat/runs/zqnkb3wv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weave.init('openrouter-chat')\n",
    "\n",
    "# Add wandb initialization\n",
    "wandb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "if wandb_key:\n",
    "    wandb.login(key=wandb_key)\n",
    "    wandb.init(project=\"openrouter-chat\")\n",
    "else:\n",
    "    print(\"WANDB_API_KEY not found. Skipping wandb initialization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d1f4b0-537c-484c-9a9a-2df860d72a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with Instructor\n",
    "openrouter_api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "openrouter_base_url = \"https://openrouter.ai/api/v1\"\n",
    "client = from_openai(OpenAI(api_key=os.environ.get(\"OPENROUTER_API_KEY\"),base_url=\"https://openrouter.ai/api/v1\"),mode=Mode.JSON)\n",
    "# client = from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a72909cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Score(BaseModel):\n",
    "    \"\"\"A score includes the value and description used to determine the metric being evaluated.\"\"\"\n",
    "    score: int = Field(..., description=\"The score of the user\")\n",
    "    determination: str = Field(..., description=\"The description that determines the score.\",\n",
    "                               examples=[\"Score 0 - There is no factual evidence to support the claim\",\n",
    "                                         \"Score 1 - There is factual evidence to support the claim\"])\n",
    "\n",
    "class Metric(BaseModel):\n",
    "    \"\"\"A metric is a set of criteria with numerical scores and descriptions used to evaluate the output of a language model. It is based on the prompt input.\"\"\"\n",
    "    metric_name: str = Field(..., description=\"The title of the metric.\")\n",
    "    metric_desc: str = Field(..., description=\"The description of the metric being used.\")\n",
    "    metric_type: str = Field(..., description=\"The type of metric being described as either a binary or graded metric.\",\n",
    "                      examples=[\"Graded (0-5)\", \"Binary (Pass/Fail)\"])\n",
    "    criteria: list[Score] = Field(..., description=\"Based on the metric, returns the set of scores that will be used to evaluate the metric.\")\n",
    "\n",
    "\n",
    "class FullMetric(BaseModel):\n",
    "    \"\"\"A list of components to evaluate an Language Model Response, which includes the description of the individual metrics, the set of criteria and scores per metric.\"\"\"\n",
    "    description: str = Field(..., description=\"A detailed description of the metric, describing the purpose and vision of an evaluation criteria.\")\n",
    "    nlp_tasks: list[str] = Field(..., description=\"A list of NLP tasks that are distinct and domain-specific\",\n",
    "                                 examples=\"Clinical NER, Text Generation Storytelling for Children, QA RAG\")\n",
    "    metric: list[Metric] = Field(..., min_length=3, max_length=10, description=\"The metric being described\",\n",
    "                                 examples=[\n",
    "                                     \"Metric 1: The response includes a factual error and scores using a Binary Criteria. Criteria: The response includes a factual error. Score: 0\",\n",
    "                                     \"Metric 2: The response is not descriptive and scores using a 5 point Likert Scale Criteria. Criteria: The response is not descriptive. Score: 1\"\n",
    "                                 ]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7236f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the travel agent to use\n",
    "from textwrap import dedent\n",
    "\n",
    "def generate_metric(user_prompt, llm_response):\n",
    "    system_content = dedent(\"\"\"\n",
    "    Instructions:\n",
    "    You are an AI assistant tasked with evaluating a response generated by an LLM (Large Language Model). \n",
    "    Responses and descriptions need to be concrete, and well-structured, effectively highlighting key points that lead towards an actionable response by the reviewer. \n",
    "    The evaluation must be tailored to the specific NLP tasks and domain expertise required by the user's prompt in a specific manner.\n",
    "    Follow the steps below to produce a comprehensive, expert-level assessment using dynamically generated metrics that are highly relevant to the task and domain at hand.\n",
    "    You have the full control over the creativity of the output, so long as it is non-inferior to the current examples listed.\n",
    "    Step 1: Analyze the Task and Domain\n",
    "    1. Identify the NLP Task Type:\n",
    "    Examine the user prompt and the LLM's response to determine the specific NLP task involved. Possible tasks include:\n",
    "    Question Answering (QA)\n",
    "    Named Entity Recognition (NER)\n",
    "    Creative Generation (e.g., storytelling, poem writing)\n",
    "    Summarization\n",
    "    Translation\n",
    "    Dialogue Generation\n",
    "    Information Extraction\n",
    "    Classification\n",
    "    2. Determine the Required Domain Expertise:\n",
    "    Identify the subject matter and domain-specific knowledge needed to address the prompt effectively. Possible domains include:\n",
    "    Medicine\n",
    "    Law\n",
    "    Literature\n",
    "    Computer Science\n",
    "    Finance\n",
    "    Psychology\n",
    "    Education\n",
    "    ---\n",
    "    Step 2: Generate Customized Evaluation Metrics\n",
    "    Based on the identified NLP task and domain expertise, generate a set of evaluation metrics that are both task-specific and domain-specific. Ensure that these metrics are defined clearly and are appropriate for an expert in the field to use as a standard.\n",
    "    A. Task-Specific Metrics\n",
    "    Define metrics that are standard for evaluating the identified NLP task.\n",
    "    Examples:\n",
    "    For Question Answering:\n",
    "    Accuracy of Answer\n",
    "    Completeness\n",
    "    Relevance\n",
    "    For NER:\n",
    "    Precision\n",
    "    Recall\n",
    "    F1 Score\n",
    "    For Creative Generation:\n",
    "    Originality\n",
    "    Creativity\n",
    "    Emotional Impact\n",
    "    B. Domain-Specific Metrics\n",
    "    Define metrics that reflect the standards and expectations of the identified domain.\n",
    "    Examples:\n",
    "    For Medicine:\n",
    "    Clinical Accuracy\n",
    "    Use of Medical Terminology\n",
    "    Adherence to Ethical Guidelines\n",
    "    For Law:\n",
    "    Legal Accuracy\n",
    "    Citation of Relevant Laws\n",
    "    Logical Consistency in Arguments\n",
    "    ---\n",
    "    Step 3: Evaluate the LLM's Response Using the Generated Metrics\n",
    "    For each metric:\n",
    "    1. Define the Metric:\n",
    "    Provide a clear and concise definition so that it is understood how the evaluation will be conducted.\n",
    "    2. Evaluation Method:\n",
    "    Graded Metrics: Use a numerical scale (e.g., 0-5) where appropriate.\n",
    "    Binary Metrics: Use Pass/Fail, Yes/No, or Compliant/Non-Compliant where a binary assessment is more suitable.\n",
    "    3. Assess the Response:\n",
    "    Apply the metric to the LLM's response.\n",
    "    Provide specific examples or evidence from the response to support your assessment.\n",
    "    4. Provide a Score or Judgment:\n",
    "    Assign a score or make a binary judgment as defined.\n",
    "    ---\n",
    "    Step 4: Provide an Overall Assessment and Recommendations\n",
    "    Summarize the Overall Quality:\n",
    "    Highlight the main strengths and weaknesses observed in the response.\n",
    "    Final Recommendations:\n",
    "    Offer actionable suggestions for improvement.\n",
    "    Suggest any additional resources or corrections needed.\n",
    "    ---\n",
    "    Template for the Evaluation Report:\n",
    "    ---\n",
    "    User Prompt:\n",
    "    [Insert User Prompt Here]\n",
    "    LLM's Response:\n",
    "    [Insert LLM's Response Here]\n",
    "    ---\n",
    "    Analysis:\n",
    "    1. Identified NLP Task Type: [Specify the task]\n",
    "    2. Identified Domain Expertise Required: [Specify the domain]\n",
    "    ---\n",
    "    Customized Evaluation Metrics:\n",
    "    Metric 1: [Name of Metric]\n",
    "    Type: [Graded (0-5) or Binary (Pass/Fail)]\n",
    "    Definition: [Provide a clear definition]\n",
    "    Assessment:\n",
    "    [Apply the metric to the response, citing specific examples]\n",
    "    Score/Judgment: [Provide the score or Pass/Fail]\n",
    "    (Repeat for each metric)\n",
    "    ---\n",
    "    Overall Assessment:\n",
    "    Summary of Findings:\n",
    "    [Summarize the key points from the evaluation]\n",
    "    Strengths:\n",
    "    [List what the response did well]\n",
    "    Areas for Improvement:\n",
    "    [List what can be improved, with suggestions]\n",
    "    Final Recommendations:\n",
    "    [Provide actionable advice]\n",
    "    ---\n",
    "    Example Evaluation Report:\n",
    "    (Below is an illustrative example using placeholders. Replace with actual content.)\n",
    "    ---\n",
    "    User Prompt:\n",
    "    \"Explain the process of mitosis in human cells.\"\n",
    "    LLM's Response:\n",
    "    \"Mitosis is the process by which a cell divides into two new cells. It consists of phases called prophase, metaphase, anaphase, and telophase. During mitosis, DNA replicates, and the cell splits equally.\"\n",
    "    ---\n",
    "    Analysis:\n",
    "    1. Identified NLP Task Type: Expository Answering\n",
    "    2. Identified Domain Expertise Required: Cell Biology\n",
    "    ---\n",
    "    Customized Evaluation Metrics:\n",
    "    Metric 1: Scientific Accuracy\n",
    "    Type: Graded (0-5)\n",
    "    Definition: Evaluates the correctness of biological facts presented.\n",
    "    Assessment:\n",
    "    The response correctly identifies mitosis as a cell division process.\n",
    "    It mentions the phases but omits cytokinesis.\n",
    "    It incorrectly states that DNA replicates during mitosis (DNA replication occurs during interphase).\n",
    "    Score: 3/5\n",
    "    Metric 2: Use of Terminology\n",
    "    Type: Binary (Pass/Fail)\n",
    "    Definition: Assesses correct use of biological terms.\n",
    "    Assessment:\n",
    "    Terms like \"prophase,\" \"metaphase,\" \"anaphase,\" and \"telophase\" are used correctly.\n",
    "    Misuse of \"DNA replicates during mitosis.\"\n",
    "    Judgment: Fail\n",
    "    Metric 3: Completeness\n",
    "    Type: Graded (0-5)\n",
    "    Definition: Measures how thoroughly the response covers the process.\n",
    "    Assessment:\n",
    "    Lacks mention of interphase and cytokinesis.\n",
    "    Does not describe what happens in each phase.\n",
    "    Score: 2/5\n",
    "    ---\n",
    "    Overall Assessment:\n",
    "    Summary of Findings:\n",
    "    The response demonstrates a basic understanding of mitosis but contains factual inaccuracies and omissions.\n",
    "    Strengths:\n",
    "    Correctly lists the main phases of mitosis.\n",
    "    Areas for Improvement:\n",
    "    Correct the misconception about DNA replication timing.\n",
    "    Include details about each phase.\n",
    "    Mention cytokinesis and its role.\n",
    "    Final Recommendations:\n",
    "    Revise the response to correct factual errors.\n",
    "    Expand on each phase to enhance completeness.\n",
    "    Ensure all biological terms are used accurately.\n",
    "    ---\n",
    "    Note: This evaluation is tailored to the specific task (explaining a biological process) and domain (cell biology), using metrics that are relevant and meaningful to experts in the field.\n",
    "    ---\n",
    "    Guidelines for Using This Prompt:\n",
    "    Adaptability: The prompt is designed to be flexible. The evaluator dynamically generates metrics based on the task and domain identified.\n",
    "    Specificity: By focusing on task-specific and domain-specific metrics, the evaluation becomes more precise and valuable.\n",
    "    Expert-Level Assessment: The metrics and evaluation criteria aim to reflect the standards expected by professionals in the relevant field.\n",
    "    Actionable Feedback: Providing detailed assessments and recommendations helps improve future LLM responses.                        \n",
    "    \"\"\")\n",
    "\n",
    "    # Use the Instructor framework for function calling\n",
    "    evaluation = client.chat.completions.create(\n",
    "        # model=\"microsoft/phi-3-mini-128k-instruct:free\",\n",
    "        # model=\"openai/gpt-4o\",\n",
    "        model=\"gpt-4o\",\n",
    "        response_model=FullMetric,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": f\"User Prompt: {user_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"LLM's Response: {llm_response}\"},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf728c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/praveenhm2/openrouter-chat/r/call/01921b71-a5b9-7652-b219-d288b880f2e7\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Given a Hypertensive CKD patient with Heart Failure and declining kidney function and hypokalemia, provide a care plan to improve their health.\"\n",
    "sample_response = \"The patient should receive a beta blockers, low-salt diet, diuretics, and potassium supplements. No further action is needed because this already addresses all their needs.\"\n",
    "\n",
    "generated_metric = generate_metric(sample_prompt, sample_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83be082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# # rprint(generated_metric)\n",
    "\n",
    "# for m in generated_metric.metric:\n",
    "#     print(\"Metric: \", m.metric_name)\n",
    "#     print(\"Metric Type: \", m.metric_type)\n",
    "#     for c in m.criteria:\n",
    "#         print(\"Score: \", c.score)\n",
    "#         print(\"Determination: \", c.determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce9f24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.metrics import list_all_metrics\n",
    "\n",
    "# list_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7b7ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.models.model_factory import ModelFactory\n",
    "# from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "# from flow_judge.metrics import RESPONSE_CORRECTNESS_BINARY\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# # Create a model using ModelFactory\n",
    "# # model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\")\n",
    "# model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# # Initialize the judge\n",
    "# judge = FlowJudge(\n",
    "#     metric=RESPONSE_CORRECTNESS_BINARY,\n",
    "#     model=model\n",
    "# )\n",
    "\n",
    "# # Prepare evaluation input\n",
    "# eval_input = EvalInput(\n",
    "#     inputs=[{\"question\": \"What is the capital of France?\"}],\n",
    "#     output=\"The capital of France is Paris.\"\n",
    "# )\n",
    "\n",
    "# # Perform evaluation\n",
    "# result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342c17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# rprint(\"Score: \", result.score)\n",
    "# rprint(\"Feedback: \", result.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ceec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flow_judge.models.model_factory import ModelFactory\n",
    "# from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "# from flow_judge.metrics import CustomMetric, RubricItem\n",
    "\n",
    "\n",
    "# # # model = ModelFactory.create_model(\"Flow-Judge-v0.1-AWQ\")\n",
    "# model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# custom_metric = CustomMetric(\n",
    "#     name=\"My Custom Metric\",\n",
    "#     criteria=\"Evaluate based on X, Y, and Z.\",\n",
    "#     rubric=[\n",
    "#         RubricItem(score=0, description=\"Poor performance\"),\n",
    "#         RubricItem(score=1, description=\"Good performance\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# judge = FlowJudge(\n",
    "#     metric=custom_metric,\n",
    "#     model=model\n",
    "# )\n",
    "\n",
    "# eval_input = EvalInput(\n",
    "#     inputs=[{\"question\": sample_prompt}],\n",
    "#     output=sample_response\n",
    "# )\n",
    "\n",
    "# result = judge.evaluate(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4828fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# rprint(\"Score: \", result.score)\n",
    "# rprint(\"Feedback: \", result.feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0460cd2-4b59-4fb9-bd9a-d6af2a970923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rich import print as rprint\n",
    "# # rprint(generated_metric)\n",
    "\n",
    "# for m in generated_metric.metric:\n",
    "#     print(\"Metric: \", m.metric)\n",
    "#     print(\"Metric Type: \", m.metric_type)\n",
    "#     for c in m.criteria:\n",
    "#         print(\"Score: \", c.score)\n",
    "#         print(\"Determination: \", c.determination)\n",
    "\n",
    "# custom_metric = CustomMetric(\n",
    "#     name=\"My Custom Metric\",\n",
    "#     criteria=\"Evaluate based on X, Y, and Z.\",\n",
    "#     rubric=[\n",
    "#         RubricItem(score=0, description=\"Poor performance\"),\n",
    "#         RubricItem(score=1, description=\"Good performance\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e0f294-6288-4815-8fbd-73ef4b5112ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_dict = generated_metric.model_dump()\n",
    "# for m in metric_dict['metric']:\n",
    "#     # rprint(m['metric_name'])\n",
    "#     rprint(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "029e3ee0-8ade-455d-bee2-9dc771ab2e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-22 13:34:56,844\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-22 13:34:57 config.py:383] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-22 13:34:57 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='flowaicom/Flow-Judge-v0.1', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 09-22 13:34:58 model_runner.py:997] Starting to load model flowaicom/Flow-Judge-v0.1...\n",
      "INFO 09-22 13:34:58 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-22 13:35:00 model_runner.py:1008] Loading model weights took 7.1659 GB\n",
      "INFO 09-22 13:35:00 gpu_executor.py:122] # GPU blocks: 2268, # CPU blocks: 682\n"
     ]
    }
   ],
   "source": [
    "from flow_judge.models.model_factory import ModelFactory\n",
    "from flow_judge.flow_judge import EvalInput, FlowJudge\n",
    "from flow_judge.metrics import CustomMetric, RubricItem\n",
    "model = ModelFactory.create_model(\"Flow-Judge-v0.1\")\n",
    "\n",
    "# Iterate through the list of metrics from FullMetric and create a list of judges\n",
    "judges = []\n",
    "for metric in generated_metric.metric:\n",
    "    # Convert the criteria into RubricItems\n",
    "    rubric = [RubricItem(score=score.score, description=score.determination) for score in metric.criteria]\n",
    "    # Create a custom metric\n",
    "    custom_metric = CustomMetric(\n",
    "        name=metric.metric_name+\" \"+metric.metric_type,\n",
    "        criteria=metric.metric_desc,\n",
    "        rubric=rubric\n",
    "    )\n",
    "    # Initialize the FlowJudge object for each custom metric\n",
    "    judge = FlowJudge(\n",
    "        metric=custom_metric,\n",
    "        model=model  # Assuming the model is pre-initialized\n",
    "    )\n",
    "    # Add the judge to the list\n",
    "    judges.append(judge)\n",
    "# Evaluate the sample input\n",
    "eval_input = EvalInput(\n",
    "    inputs=[{\"question\": sample_prompt}],\n",
    "    output=sample_response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "679bfaf6-39d5-433a-b6a6-91dd3cbd2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà| 1/1 [00:06<00:00,  6.05s/it, est. speed input: 121.87 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Clinical Accuracy Graded (0-5)\n",
      "Score: 1\n",
      "Feedback: The response provided contains significant errors and does not align with current clinical guidelines for managing a patient with hypertensive chronic kidney disease (CKD), heart failure, declining kidney function, and hypokalemia. \n",
      "\n",
      "1. **Beta Blockers**: While beta blockers are generally recommended for heart failure, the choice of specific beta blocker and dosage should be carefully considered, especially in the context of declining kidney function. The response does not provide this level of detail.\n",
      "\n",
      "2. **Low-Salt Diet**: This is appropriate for managing hypertension and heart failure.\n",
      "\n",
      "3. **Diuretics**: The use of diuretics in this patient is problematic. Diuretics can exacerbate kidney function decline and worsen hypokalemia. The response does not address the need to carefully select or adjust diuretic therapy.\n",
      "\n",
      "4. **Potassium Supplements**: This is appropriate given the patient's hypokalemia. However, the response does not address the need to monitor and adjust potassium levels carefully, especially in the context of declining kidney function.\n",
      "\n",
      "The response also fails to mention other important aspects of care for this complex patient, such as:\n",
      "- Careful monitoring of kidney function and electrolytes\n",
      "- Adjustment of medications based on kidney function\n",
      "- Management of fluid status\n",
      "- Use of ACE inhibitors or ARBs, which are beneficial in CKD and heart failure but require careful monitoring of kidney function and potassium levels\n",
      "- Consideration of other medications like aldosterone antagonists\n",
      "\n",
      "Overall, the response demonstrates a lack of understanding of the complexities involved in managing this patient's condition and does not align with current clinical guidelines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà| 1/1 [00:05<00:00,  5.86s/it, est. speed input: 124.97 toks/s, outpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Comprehensiveness Graded (0-5)\n",
      "Score: 1\n",
      "Feedback: The response fails to comprehensively address the complex medical condition presented in the question. While it mentions some relevant treatments (beta blockers, low-salt diet, diuretics, and potassium supplements), it overlooks several critical aspects of care for a patient with hypertensive chronic kidney disease (CKD), heart failure, declining kidney function, and hypokalemia.\n",
      "\n",
      "Key omissions include:\n",
      "\n",
      "1. Specific medication adjustments: The response doesn't address the need for careful management of medications, particularly those that could further harm kidney function or exacerbate heart failure.\n",
      "\n",
      "2. Fluid management: The response doesn't mention the importance of careful fluid balance in a patient with heart failure and declining kidney function.\n",
      "\n",
      "3. Anemia management: The response doesn't address the common issue of anemia in CKD patients, which often requires treatment.\n",
      "\n",
      "4. Bone health: The response doesn't mention the need for monitoring and managing bone health, which can be compromised in CKD.\n",
      "\n",
      "5. Dialysis considerations: The response doesn't address when or if dialysis might become necessary as kidney function declines.\n",
      "\n",
      "6. Cardiovascular risk reduction: The response doesn't mention strategies for reducing cardiovascular risk, which is crucial in this patient population.\n",
      "\n",
      "7. Patient education: The response doesn't address the importance of educating the patient about their condition and self-management strategies.\n",
      "\n",
      "While the response does mention some relevant treatments, it fails to provide a comprehensive care plan that addresses all aspects of this complex medical condition. The statement \"No further action is needed\" is particularly concerning given the multiple serious health issues presented in the question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà| 1/1 [00:05<00:00,  5.78s/it, est. speed input: 127.36 toks/s, outpu"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Adherence to Medical Guidelines Graded (0-5)\n",
      "Score: 1\n",
      "Feedback: The response provided in the output does not fully adhere to established medical guidelines for managing a patient with hypertensive chronic kidney disease (CKD), heart failure, declining kidney function, and hypokalemia. While some elements mentioned are generally appropriate, there are significant deviations and omissions that need to be addressed:\n",
      "\n",
      "1. **Beta-blockers**: While beta-blockers are recommended for heart failure management, the specific choice and dosing should be carefully considered, especially in the context of declining kidney function. The response does not address this nuance.\n",
      "\n",
      "2. **Low-salt diet**: This is appropriate for managing hypertension and heart failure.\n",
      "\n",
      "3. **Diuretics**: This recommendation is problematic given the patient's declining kidney function and hypokalemia. Diuretics, especially loop diuretics, can exacerbate kidney injury and electrolyte imbalances. The response fails to address the need for careful diuretic management or alternative strategies.\n",
      "\n",
      "4. **Potassium supplements**: This is inappropriate given the patient's hypokalemia. The response incorrectly suggests potassium supplements, which could worsen the patient's condition.\n",
      "\n",
      "5. **No further action**: This statement is overly simplistic and fails to address the complexity of the patient's condition. A comprehensive care plan should include monitoring of kidney function, electrolytes, and heart function, as well as potential adjustments to medications and other interventions.\n",
      "\n",
      "Overall, the response shows minimal adherence to medical guidelines and fails to provide a comprehensive and safe care plan for this complex patient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation for each judge\n",
    "for judge in judges:\n",
    "    result = judge.evaluate(eval_input)\n",
    "    print(f\"Metric: {judge.metric.name}\")\n",
    "    print(f\"Score: {result.score}\")\n",
    "    print(f\"Feedback: {result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc34ff1-c9b0-4f55-915e-e99c91f4405a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
